{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c288e80",
   "metadata": {},
   "source": [
    "# SageMaker Model Deployment Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300a702",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to deploy a model using different SageMaker deployment options: Real-Time Endpoint, Serverless Inference, Batch Transform, Asynchronous Inference, and Multi-Model Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f80705",
   "metadata": {},
   "source": [
    "## Initializers and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33710e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# role = get_execution_role()\n",
    "# region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for storing data\n",
    "bucket = 'sagemaker-ml3'\n",
    "prefix = 'deployment'\n",
    "output_path = f's3://{bucket}/{prefix}/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e2d27",
   "metadata": {},
   "source": [
    "## Data Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Employee.csv'  # Replace with your actual file path in S3 if needed\n",
    "employee_df = pd.read_csv(file_path)\n",
    "employee_df.head()\n",
    "\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Convert categorical columns to numeric\n",
    "employee_df['Education'] = employee_df['Education'].astype('category').cat.codes\n",
    "employee_df['City'] = employee_df['City'].astype('category').cat.codes\n",
    "employee_df['Gender'] = employee_df['Gender'].astype('category').cat.codes\n",
    "employee_df['EverBenched'] = employee_df['EverBenched'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Drop rows with NaN values in the target column\n",
    "employee_df.dropna(subset=['LeaveOrNot'])\n",
    "\n",
    "# Convert target column to numeric if needed\n",
    "employee_df['LeaveOrNot'] = employee_df['LeaveOrNot'].astype(int)\n",
    "\n",
    "# Ensure no missing values in feature columns\n",
    "employee_df = employee_df.dropna()\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(employee_df.dtypes)\n",
    "\n",
    "# Define features and target\n",
    "feature_columns = [\n",
    "    'Education', 'JoiningYear', 'City', 'PaymentTier', 'Age',\n",
    "    'Gender', 'EverBenched', 'ExperienceInCurrentDomain'\n",
    "]\n",
    "target_column = 'LeaveOrNot'\n",
    "\n",
    "employee_df = employee_df[[target_column] + feature_columns]\n",
    "train_df, test_df = train_test_split(employee_df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 3: Upload data into S3\n",
    "import boto3\n",
    "s3 = boto3.client('s3')  # Initialize S3 client\n",
    "\n",
    "# Save the data locally first\n",
    "train_file = 'train.csv'\n",
    "validation_file = 'validation.csv'\n",
    "train_df.to_csv(train_file, index=False)\n",
    "test_df.to_csv(validation_file, index=False)\n",
    "\n",
    "# Upload the data to S3\n",
    "s3.upload_file(train_file, bucket, f'{prefix}/train/{train_file}')\n",
    "s3.upload_file(validation_file, bucket, f'{prefix}/validation/{validation_file}')\n",
    "\n",
    "print(f\"Training data uploaded to s3://{bucket}/{prefix}/train/{train_file}\")\n",
    "print(f\"Validation data uploaded to s3://{bucket}/{prefix}/validation/{validation_file}\")\n",
    "\n",
    "# Display the transformed dataset\n",
    "employee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50e267-39f2-45f3-bc39-b4065b6bf0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an XGBoost Model using SageMaker\n",
    "import sagemaker\n",
    "\n",
    "# Setup XGBoost Estimator\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"1.3-1\")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"40\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.7\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"50\"\n",
    "}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    volume_size=5,  # 5 GB \n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "# Define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = sagemaker.inputs.TrainingInput(f\"s3://{bucket}/{prefix}/train/{train_file}\", content_type=content_type)\n",
    "validation_input = sagemaker.inputs.TrainingInput(f\"s3://{bucket}/{prefix}/validation/{validation_file}\", content_type=content_type)\n",
    "\n",
    "# Execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e2420",
   "metadata": {},
   "source": [
    "## Deploy as Real-Time Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deploy the model as a real-time endpoint\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,  # Number of instances to deploy\n",
    "    instance_type='ml.m5.xlarge',  # Instance type for the endpoint\n",
    "    endpoint_name='employee-attrition-predictor'  # Name of the endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbf3fb-861d-4d5c-a1d4-40d89a5c270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, configure the predictor for the specific input and output formats\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "\n",
    "\n",
    "# Example input data (in the same format as your training data)\n",
    "\n",
    "# Assuming 'LeaveOrNot' is the target column\n",
    "features_df = test_df.drop(columns=['LeaveOrNot'])\n",
    "\n",
    "# Convert the features DataFrame to CSV format\n",
    "test_csv = features_df.to_csv(index=False, header=False).strip()\n",
    "\n",
    "test_data = test_df[feature_columns].head(20)  # Select the first row of test data for prediction\n",
    "test_data_csv = test_data.to_csv(index=False, header=False).strip()  # Convert to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b729cc8-a966-428f-ac30-065984a74b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = predictor.predict(test_data_csv)\n",
    "print(response)  # The response will be in JSON format, containing the predicted label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608e676-325f-4317-b4f5-8dd354262497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input data (in the same format as your training data)\n",
    "test_data = test_df[feature_columns].head(1)  # Select the first row of test data for prediction\n",
    "test_data_csv = test_data.to_csv(index=False, header=False).strip()  # Convert to CSV format\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = predictor.predict(test_data_csv)\n",
    "print(response)  # The response will be in JSON format, containing the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f14f7-d30d-434f-b29c-f7cc2c922c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint when no longer needed\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a91160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4f9f795",
   "metadata": {},
   "source": [
    "## Deploying Endpoint using Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e31ae-4698-4c97-9904-486659a6e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "# Specify the S3 path to the pre-trained model artifact\n",
    "model_artifact = \"s3://<your-path>/output/model.tar.gz\"\n",
    "\n",
    "# Retrieve the container image for the framework (e.g., XGBoost)\n",
    "container = sagemaker.image_uris.retrieve(framework=\"xgboost\", region=boto3.Session().region_name, version=\"1.3-1\")\n",
    "\n",
    "# Create the model object using the S3 path\n",
    "model = Model(\n",
    "    image_uri=container,\n",
    "    model_data=model_artifact,\n",
    "    role=sagemaker.get_execution_role()\n",
    ")\n",
    "\n",
    "# Deploy the model as a real-time endpoint\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,  # Number of instances\n",
    "    instance_type='ml.m5.xlarge',  # Instance type\n",
    "    endpoint_name='employee-attrition-predictor'  # Name of the endpoint\n",
    ")\n",
    "\n",
    "# Optionally, configure the predictor for the specific input and output formats\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "# Invoke the endpoint for predictions\n",
    "response = predictor.predict(test_data_csv)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e40b9-29f4-486f-89c6-5d6832e4d224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Specify the endpoint name\n",
    "endpoint_name = 'employee-attrition-predictor'\n",
    "\n",
    "# Prepare your input data (same format as before)\n",
    "test_data = test_df[feature_columns].head(25)  # Select the first row of test data for prediction\n",
    "test_data_csv = test_data.to_csv(index=False, header=False).strip()  # Convert to CSV format\n",
    "\n",
    "# Invoke the endpoint directly using the runtime client\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",  # Specify the content type\n",
    "    Body=test_data_csv  # The input data as a CSV string\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1449b-c154-4356-98ec-3c8c1268cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint when no longer needed\n",
    "predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d5765",
   "metadata": {},
   "source": [
    "## Deploy as Serverless Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe58f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Create a Model object using the trained estimator\n",
    "model = Model(\n",
    "    image_uri=xgboost_container,\n",
    "    model_data=estimator.model_data,\n",
    "    role=sagemaker.get_execution_role()\n",
    ")\n",
    "\n",
    "# Alternative\n",
    "# Create the model object using the S3 path\n",
    "# model = Model(\n",
    "#     image_uri=container,\n",
    "#     model_data=model_artifact,\n",
    "#     role=get_execution_role()\n",
    "# )\n",
    "\n",
    "# Define the Serverless Inference configuration\n",
    "serverless_inference_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,  # Allocate memory\n",
    "    max_concurrency=5  # Max concurrent invocations\n",
    ")\n",
    "\n",
    "# Deploy the model as a serverless endpoint\n",
    "serverless_predictor = model.deploy(\n",
    "    serverless_inference_config=serverless_inference_config,\n",
    "    endpoint_name='employee-attrition-serverless-1'\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a0526-207e-475f-8042-db8fe0f63781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Specify your serverless endpoint name\n",
    "endpoint_name = \"employee-attrition-serverless-1\"  # Replace with your actual endpoint name\n",
    "\n",
    "# Specify the content type\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "# Use the CSV string from test_df as the payload\n",
    "payload = test_data_csv\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "# Read and decode the response\n",
    "result = response['Body'].read().decode('utf-8').splitlines()\n",
    "\n",
    "# Print the predictions\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f42f5c-4cf1-4042-8b29-601457c0dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result))  # Should match len(test_df)\n",
    "\n",
    "print(test_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c471b4-775c-4849-b2c8-bef2fbf07861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input data (in the same format as your training data)\n",
    "test_data = test_df[feature_columns].head(23)  # Select the first row of test data for prediction\n",
    "test_data_csv = test_data.to_csv(index=False, header=False).strip()  # Convert to CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506830e3",
   "metadata": {},
   "source": [
    "## Deploy using Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74995d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.transformer import Transformer  # Import the Transformer class\n",
    "\n",
    "# S3 bucket for storing data\n",
    "# bucket = 'sagemaker-ml-28573'\n",
    "# prefix = 'deployment'\n",
    "\n",
    "output_data = f's3://{bucket}/{prefix}/output/'\n",
    "\n",
    "\n",
    "# Specify the S3 path to the pre-trained model artifact\n",
    "# model_artifact = \"s3://sagemaker-ml-28573/demo-built-in-algorithm/output/sagemaker-xgboost-2024-08-29-13-30-18-037/output/model.tar.gz\"\n",
    "\n",
    "\n",
    "# Retrieve the container image for the framework (e.g., XGBoost)\n",
    "container = sagemaker.image_uris.retrieve(framework=\"xgboost\", region=boto3.Session().region_name, version=\"1.3-1\")\n",
    "\n",
    "\n",
    "# Create the model object using the S3 path\n",
    "model = Model(\n",
    "    image_uri=container,\n",
    "    model_data=model_artifact,\n",
    "    role=sagemaker.get_execution_role()\n",
    ")\n",
    "\n",
    "\n",
    "# Create a transformer object\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',  # Choose an instance type\n",
    "    output_path=output_data,\n",
    "    strategy='MultiRecord',  # Strategy for processing records (SingleRecord or MultiRecord)\n",
    "    assemble_with='Line',  # How to join results, e.g., 'Line' to join with newlines\n",
    "    accept='text/csv'  # Output format\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Start the transform job\n",
    "transformer.transform(\n",
    "    data=input_data,  # Input data in S3\n",
    "    content_type='text/csv',  # Input format\n",
    "    split_type='Line'  # How the input data is split (e.g., by line)\n",
    ")\n",
    "\n",
    "# Wait for the job to finish\n",
    "transformer.wait()\n",
    "\n",
    "\n",
    "\n",
    "# The results will be available in the S3 output path specified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0505528",
   "metadata": {},
   "source": [
    "## Deploy as Asynchronous Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "# Deploy the model as an asynchronous endpoint\n",
    "async_predictor = estimator.deploy(\n",
    "    initial_instance_count=1,  # Number of instances\n",
    "    instance_type='ml.m5.xlarge',  # Instance type\n",
    "    async_inference_config=AsyncInferenceConfig(\n",
    "        output_path=f's3://{bucket}/{prefix}/async-output',  # S3 path to store output\n",
    "        max_concurrent_invocations_per_instance=2\n",
    "    ),\n",
    "    endpoint_name='employee-attrition-async-1'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfe936-4850-48c7-acf0-41253c5d1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the label column and the header\n",
    "features_only = test_df.iloc[:, 1:]  # Exclude the first column (label)\n",
    "test_data_csv = features_only.to_csv(index=False, header=False).strip()\n",
    "\n",
    "# Save to CSV without header\n",
    "csv_file_path = 'validation_no_label.csv'\n",
    "features_only.to_csv(csv_file_path, index=False, header=False)\n",
    "\n",
    "# Upload the prepared CSV to S3\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(csv_file_path, bucket, f'{prefix}/validation/validation_no_label.csv')\n",
    "\n",
    "# Update the input location for the asynchronous request\n",
    "input_location = f's3://{bucket}/{prefix}/validation/validation_no_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2706802-ddbe-4f46-826d-7f77d7f6d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "\n",
    "\n",
    "\n",
    "# The name of the asynchronous endpoint that you have deployed\n",
    "\n",
    "endpoint_name = 'employee-attrition-async-1'\n",
    "\n",
    "# Invoke the asynchronous endpoint with your input data location\n",
    "response = sagemaker_runtime.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=input_location,\n",
    "    InvocationTimeoutSeconds=1600,  # Set timeout to allow sufficient time for processing\n",
    "    ContentType='text/csv'\n",
    ")\n",
    "\n",
    "# Response contains metadata about the request, not the prediction itself.\n",
    "# The actual predictions will be saved to the specified S3 output path.\n",
    "print(\"Asynchronous inference request sent. Check S3 for results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962391b-a071-4dab-b0b9-a81e7a4fb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f's3://{bucket}/{prefix}/validation/validation_no_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9772496-d4b2-47e9-91e2-736f01b2aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cc474-2868-422f-b051-ff24b74290e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the asynchronous endpoint\n",
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdcc04",
   "metadata": {},
   "source": [
    "## Deploy as Multi-Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Model\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "# S3 path to your model artifacts\n",
    "model_artifact_path = 's3://sagemaker-ml-28573/deployment/output/sagemaker-xgboost-2024-09-03-12-15-08-548/output/'\n",
    "\n",
    "# Define the container image (for example, XGBoost)\n",
    "container_image = sagemaker.image_uris.retrieve(framework=\"xgboost\", region=boto3.Session().region_name, version=\"1.3-1\")\n",
    "\n",
    "# Create a Model object (this includes the image and role information)\n",
    "model = Model(\n",
    "    image_uri=container_image,\n",
    "    role=sagemaker.get_execution_role()\n",
    ")\n",
    "\n",
    "# Create the MultiDataModel\n",
    "mme = MultiDataModel(\n",
    "    name=\"multi-model-endpoint\",\n",
    "    model_data_prefix=model_artifact_path,  # S3 path where models are stored\n",
    "    model=model,  # Pass the Model object that defines the container image\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "# Deploy the Multi-Model Endpoint\n",
    "predictor = mme.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    endpoint_name=\"multi-model-endpoint\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49506f33-9124-4a94-848c-d6d918e182c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Initialize the SageMaker Runtime client with a retry strategy\n",
    "config = Config(\n",
    "    read_timeout=70,\n",
    "    retries={\n",
    "        'max_attempts': 2  # Adjust this value as needed (up to 5 for a max timeout of 360s)\n",
    "    }\n",
    ")\n",
    "runtime_sagemaker_client = boto3.client('sagemaker-runtime', config=config)\n",
    "\n",
    "# Define the endpoint name and the specific model to target\n",
    "endpoint_name = \"multi-model-endpoint\"\n",
    "target_model = \"model.tar.gz\"\n",
    "\n",
    "# Example CSV input data\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = runtime_sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    TargetModel=target_model,\n",
    "    Body=test_data_csv\n",
    ")\n",
    "\n",
    "# Print the response from the model\n",
    "print(response['Body'].read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d38b45-b387-492c-95ae-4e55b748d27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7034a-f063-402e-8170-5d0fc9a0ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the multi-model endpoint\n",
    "multi_model_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
